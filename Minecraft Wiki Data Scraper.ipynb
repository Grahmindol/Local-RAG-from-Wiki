{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Wiki Loader Notebook\n",
    "This notebook demonstrates how to fetch and process data from the French Minecraft Wiki using Python. It contains various functions and a custom loader class for extracting and processing wiki data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "The libraries used in this notebook include:\n",
    "- `requests` for making HTTP requests\n",
    "- `BeautifulSoup` for parsing HTML content\n",
    "- Classes from `langchain_core` for handling documents and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from rich.progress import Progress\n",
    "import json\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import ollama\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Revision IDs\n",
    "The `fetch_rvid` function retrieves the revision ID of a wiki page as of a specified date. This is essential for accessing historical content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"MyScript/1.0 (myemail@example.com)\"}\n",
    "\n",
    "def fetch_rvid(title, date=\"2021-01-01T00:00:00.000Z\", api = \"https://fr.minecraft.wiki/api.php\"):\n",
    "    # API request parameters\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": title,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"rvprop\": \"ids\",\n",
    "        \"rvlimit\": \"1\",\n",
    "        \"rvstart\": date,\n",
    "        \"rvdir\": \"older\"\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(api, params=params, headers=headers)\n",
    "\n",
    "    # Check if the response was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the revisions data safely\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", [])\n",
    "        if pages:\n",
    "            page_info = pages[0]\n",
    "            revisions = page_info.get(\"revisions\", [])\n",
    "            \n",
    "            if revisions:\n",
    "                # Return the revision ID if found\n",
    "                return revisions[0].get(\"revid\", None)\n",
    "    \n",
    "    # Return None if no revisions are found or if there's an issue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Category Members\n",
    "The `fetch_category_members` function retrieves the titles of all pages within a specified category. It handles pagination if the number of pages exceeds the API's limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_category_members(category, limit=500 , api = \"https://fr.minecraft.wiki/api.php\"):\n",
    "    members = []\n",
    "    cmcontinue = None\n",
    "\n",
    "    while True:\n",
    "        # API request parameters\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": category,\n",
    "            \"cmlimit\": limit,\n",
    "            \"format\": \"json\",\n",
    "            \"cmtype\": \"page\",\n",
    "        }\n",
    "        if cmcontinue:\n",
    "            params[\"cmcontinue\"] = cmcontinue\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(api, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        # Collect members\n",
    "        members.extend([page[\"title\"] for page in data.get(\"query\", {}).get(\"categorymembers\", [])])\n",
    "        \n",
    "        # Check if more pages are available\n",
    "        cmcontinue = data.get(\"continue\", {}).get(\"cmcontinue\")\n",
    "        if not cmcontinue:\n",
    "            break\n",
    "    return members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Page Content\n",
    "The `fetch_page_content` function fetches the HTML content of a wiki page at a specific revision. It extracts and prettifies the content using `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_content(title : str,rvid = \"\", vrb = True, baseURL = \"https://fr.minecraft.wiki\"):\n",
    "    page_url = f\"{baseURL}/w/{title.replace(' ', '_')}?oldid={rvid}\"\n",
    "    if vrb:\n",
    "        print(f\"collecting data from : {page_url}\")\n",
    "    \n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Unable to fetch the page. Status code {response.status_code}\")\n",
    "        return page_url, None\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    soup.prettify(formatter=\"html5\")\n",
    "    \n",
    "    return  page_url, soup.select_one('#mw-content-text') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions\n",
    "These utility functions determine whether a paragraph is worth extracting (`is_textable`) and clean up the text (`textify`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_textable(p : BeautifulSoup):\n",
    "    text = p.get_text()\n",
    "    if(not text):\n",
    "       return False\n",
    "    length = len( text.strip())\n",
    "    return  ((length > 30) and # skip things like \"Alambic/BS\" witch is useless\n",
    "            (not text.strip().endswith(\":\")) and # skip <p> follawed by array or image\n",
    "            (not text.strip().startswith(\"Erreur\"))  # skip error message <p>\n",
    "            (not \"bug tracker\" in text)\n",
    "         )\n",
    "\n",
    "def to_ascii(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode()\n",
    "\n",
    "def textify(p : BeautifulSoup):\n",
    "   input_text = p.get_text()\n",
    "   input_text = re.sub(r'\\s+', ' ', input_text)\n",
    "   input_text = re.sub(r'\\[([a-zA-Z0-9])\\]', '', input_text)\n",
    "   return process(to_ascii(input_text))\n",
    "\n",
    "\n",
    "def process(input_text: str):\n",
    "    prompt = f\"\"\"\n",
    "Rephrase the following text into clear, simple, independent sentences.\n",
    "- Each sentence must contain only one subject.\n",
    "- Replace all pronouns like \"they\", \"this\", \"other\", \"the\" or \"it\" with explicit nouns.\n",
    "- If a sentence contains multiple actions, split it into separate sentences, one action per sentence.\n",
    "- Do not add or remove any information.\n",
    "- Sentences should be fully independent; none should require context from another.\n",
    "- Ignore line breaks in the input; split sentences based on content and meaning, not line endings.\n",
    "- Write each sentence on its own line without punctuation at the end.\n",
    "\n",
    "Example:\n",
    "Input text:\n",
    "Certain non-hostile mobs such as chickens skeleton horses and spiders can be mounted by hostile mobs on rare occasions and become hostile\n",
    "\n",
    "Output:\n",
    "Chickens are non-hostile mobs\n",
    "Skeleton horses are non-hostile mobs\n",
    "Spiders are non-hostile mobs\n",
    "Hostile mobs can mount Chickens on rare occasions\n",
    "Hostile mobs can mount Skeleton horses on rare occasions\n",
    "Hostile mobs can mount Spiders on rare occasions\n",
    "Mounted mobs become hostile\n",
    "\n",
    "Now rephrase this text:\n",
    "{input_text}\"\"\"\n",
    "    text = ollama.generate(model='qwen2.5', prompt=prompt).response.split(\"</think>\", 1)\n",
    "    return [t.strip() for t in text[-1].split('\\n') if t.strip() != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mobs spawn in various ways',\n",
       " 'Most mobs spawn naturally',\n",
       " 'Light level affects mob spawning',\n",
       " 'Biome influences mob presence',\n",
       " 'Surroundings impact mob appearance',\n",
       " 'Animals are typically found in bright areas on the surface',\n",
       " 'Hostile monsters commonly appear in dark places',\n",
       " 'Dark places include caves, monster rooms, mansions, or at night',\n",
       " 'Animals usually spawn upon chunk generation',\n",
       " 'Hostile monsters spawn and despawn within a certain radius around the player']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(\"Mobs spawn in various ways. Most mobs spawn naturally, depending on the light level, biome, and their surroundings. For example, most animals are found in bright areas on the surface, while hostile monsters are commonly found in the dark (whether it's a cave, monster room, mansion, or at night). Animals usually spawn upon chunk generation, while hostile monsters spawn and despawn in a certain radius around the player.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader Class\n",
    "The `MinecraftWikiLoader` class processes wiki pages within specified categories. It lazily loads paragraphs as `Document` objects for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinecraftWikiLoader(BaseLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorys=[\"Category:Blocks\", \"Category:Environment\", \"Category:Gameplay\", \"Category:Redstone\", \"Category:Entities\"],\n",
    "        date=\"2021-01-01T00:00:00.000Z\",\n",
    "        lang=\"en\"\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the loader by fetching and processing content from the Minecraft wiki.\"\"\"\n",
    "        self.documents = []  # Will store all Document objects\n",
    "        self.baseURL = f\"https://{lang}.minecraft.wiki\"\n",
    "        self.api = f\"{self.baseURL}/api.php\"\n",
    "\n",
    "        # --- Fetch pages from categories ---\n",
    "        print(\"Getting all pages...\")\n",
    "        pages = []\n",
    "        for cat in categorys:\n",
    "            for page in fetch_category_members(cat, api=self.api):\n",
    "                pages.append(page)\n",
    "        pages = list(set(pages))  # remove duplicates\n",
    "\n",
    "        self.categorys_page = []\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\"[cyan]Sorting Pages...\", total=len(pages))\n",
    "            for page in pages:\n",
    "                rvid = fetch_rvid(page, date=date, api=self.api)\n",
    "                if rvid:\n",
    "                    self.categorys_page.append({\"title\": page, \"id\": rvid})\n",
    "                progress.update(task, advance=1)\n",
    "            progress.stop_task(task)\n",
    "\n",
    "        # --- Fetch content for each page and store as documents ---\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\"[cyan]Getting Pages...\", total=len(self.categorys_page))\n",
    "            for page in self.categorys_page:\n",
    "                page_url, page_content = fetch_page_content(\n",
    "                    page[\"title\"], rvid=page[\"id\"], vrb=False, baseURL=self.baseURL\n",
    "                )\n",
    "                if page_content:\n",
    "                    # Select paragraphs from the page content\n",
    "                    paragraphs = page_content.select('div.mw-parser-output > p')\n",
    "                    for p in paragraphs:\n",
    "                        if is_textable(p):\n",
    "                            self.documents.append(\n",
    "                                {\n",
    "                                    \"page_content\": re.sub(r'\\s+', ' ', p.get_text().strip()),  # Clean up text efficiently\n",
    "                                    \"metadata\": {\"title\": page[\"title\"], \"source\": page_url}\n",
    "                                }\n",
    "                            )\n",
    "                    \n",
    "                progress.update(task, advance=1)\n",
    "            progress.stop_task(task)\n",
    "        with open('data.json', 'w') as f:\n",
    "            json.dump(self.documents, f, indent=4)\n",
    "        self.page_count = len(self.documents)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file_path=\"data.json\") -> \"MinecraftWikiLoader\":\n",
    "        instance = cls.__new__(cls)\n",
    "        with open(file_path, 'r') as f:\n",
    "            instance.documents = json.load(f)\n",
    "        instance.page_count = len(instance.documents)\n",
    "        return instance\n",
    "\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"Yields the pre-fetched Document objects one by one.\"\"\"\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\"[cyan]Processing data...\", total=len(self.documents))\n",
    "            for doc in self.documents:\n",
    "                for s in process(to_ascii(doc[\"page_content\"])):#process(doc[\"page_content\"]):  # Access content correctly\n",
    "                    yield Document(\n",
    "                        page_content=s,\n",
    "                        metadata=doc[\"metadata\"]  # Access metadata correctly\n",
    "                    )\n",
    "                progress.update(task, advance=1)\n",
    "            progress.stop_task(task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader Initialization and Testing\n",
    "Here, the `MinecraftWikiLoader` is initialized and tested by collecting the first 20 paragraphs from the specified categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noeay\\AppData\\Local\\Temp\\ipykernel_10852\\3354370599.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  db_data = Chroma(persist_directory= \"./wiki_db\",embedding_function= OllamaEmbeddings(model=model_name),collection_name=\"data\")\n",
      "C:\\Users\\noeay\\AppData\\Local\\Temp\\ipykernel_10852\\3354370599.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db_data = Chroma(persist_directory= \"./wiki_db\",embedding_function= OllamaEmbeddings(model=model_name),collection_name=\"data\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mxbai-embed-large\"\n",
    "db_data = Chroma(persist_directory= \"./wiki_db\",embedding_function= OllamaEmbeddings(model=model_name),collection_name=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6720"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader =  MinecraftWikiLoader.from_file() # peut prendre un moment le temp de trié les page.... 3 min en général\n",
    "loader.page_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ab0af471744665aeed48bfc4fce194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for document in loader.lazy_load():\n",
    "    try:\n",
    "        db_data.add_documents([document])\n",
    "    except: \n",
    "        print(\"Error on : \", document)\n",
    "        \n",
    "db_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_data.as_retriever(search_kwargs={\"k\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"they\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "\n",
    "def schearch(query):\n",
    "    data = retriever.invoke(query)\n",
    "    data_result = ''.join([i.page_content+'\\n' for i in data])\n",
    "    print(data_result)\n",
    "    return ollama.generate(model = 'qwen2.5', prompt=f\"You are an API that summarizes data relevant to answering the request. If there is no useful data, respond with 'No data found.' \\n Request: {query} \\n Data: {data_result} \\n Provide a concise and relevant summary based on the available data.\").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
